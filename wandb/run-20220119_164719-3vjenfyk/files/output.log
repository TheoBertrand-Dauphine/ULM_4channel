GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Validation sanity check:   0%|                                                                                                                                                       | 0/1 [00:00<?, ?it/s]
   | Name       | Type            | Params
------------------------------------------------
0  | encoder1   | Sequential      | 2.5 K
1  | pool1      | MaxPool2d       | 0
2  | encoder2   | Sequential      | 14.0 K
3  | pool2      | MaxPool2d       | 0
4  | encoder3   | Sequential      | 55.6 K
5  | pool3      | MaxPool2d       | 0
6  | encoder4   | Sequential      | 221 K
7  | pool4      | MaxPool2d       | 0
8  | bottleneck | Sequential      | 885 K
9  | upconv4    | ConvTranspose2d | 131 K
10 | decoder4   | Sequential      | 442 K
11 | upconv3    | ConvTranspose2d | 32.8 K
12 | decoder3   | Sequential      | 110 K
13 | upconv2    | ConvTranspose2d | 8.2 K
14 | decoder2   | Sequential      | 27.8 K
15 | upconv1    | ConvTranspose2d | 2.1 K
16 | decoder1   | Sequential      | 7.0 K
17 | conv       | Conv2d          | 51
------------------------------------------------
1.9 M     Trainable params
0         Non-trainable params
1.9 M     Total params
7.769     Total estimated model params size (MB)
/home/tbertrand/Bureau/ULM_4channel_pull/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/tbertrand/Bureau/ULM_4channel_pull/None/version_None/checkpoints exists and is not empty.
Validation sanity check:   0%|                                                                                                                                                       | 0/1 [00:00<?, ?it/s]torch.Size([3950, 4])
195
186
185
158
221
178
176
175
179
164
155
182
263
195
154
180
207
172
475
150
Epoch 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:01<00:00, 10.22it/s, loss=0.182]
Validating:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?it/s]
/home/tbertrand/Bureau/ULM_4channel_pull/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:428: UserWarning: The number of training samples (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
187idating:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?it/s]
173
163
184
167
156
161
187
167
172
148
163
164
187
179
145
180
159
186
177
Epoch 1:  36%|████████████████████████████████████████████████▋                                                                                     | 4/11 [00:00<00:01,  5.67it/s, loss=0.171, v_num=nfyk]
/home/tbertrand/Bureau/ULM_4channel_pull/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
Epoch 1:  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉            | 10/11 [00:01<00:00,  8.84it/s, loss=0.157, v_num=nfyk]torch.Size([5560, 4])
360idating:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?it/s]
386
250
247
175
236
239
244
234
236
210
359
196
343
290
278
416
320
239
302
Epoch 2:  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉            | 10/11 [00:01<00:00,  8.27it/s, loss=0.117, v_num=nfyk]
520idating:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?it/s]
510
383
498
304
410
526
555
406
523
371
524
260
553
500
352
542
451
240
469
Epoch 3:  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 10/11 [00:01<00:00,  9.10it/s, loss=0.0918, v_num=nfyk]
505idating:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?it/s]
490
388
494
341
436
556
571
442
535
398
523
301
576
505
350
555
452
251
488
Epoch 4:  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 10/11 [00:01<00:00,  9.03it/s, loss=0.0721, v_num=nfyk]torch.Size([9460, 4])
516idating:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?it/s]
514
427
513
325
491
546
557
463
516
440
540
306
586
519
364
558
494
277
508
Epoch 4: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:02<00:00,  4.32it/s, loss=0.0721, v_num=nfyk]